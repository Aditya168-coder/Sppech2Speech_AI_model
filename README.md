
# Speech2Speech AI Model

This repository presents multiple methods to train your own **Speech-to-Speech (S2S)** model. 

---

## ðŸ”§ Model Fine-Tuning with Unsloth

We fine-tuned the LLaMA model using [Unsloth](https://github.com/unslothai/unsloth) for efficiency and stability.

### Why we used Unsloth:

* âœ… **Optimized for speed**: It significantly reduces training time.
* âœ… **Memory efficient**: Supports longer sequences and larger batch sizes on limited hardware.
* âœ… **Seamless integration** with Hugging Face and LLaMA models.
* âœ… **Low-Rank Adaptation (LoRA)** support for faster fine-tuning with fewer parameters.

---

## ðŸ§ª Experiments & Hypotheses

We explored three different hypotheses to build the speech-to-speech pipeline:

---

### **Method 1: Whisper â†’ Audio Encoder â†’ LLaMA â†’ gTTS**

* **Pipeline**:
  Whisper â†’ Audio Encoder (Embeddings â†’ Tokens) â†’ Fine-tuned LLaMA â†’ gTTS

* **Goal**: Convert audio to embeddings, translate via LLaMA, and convert back to audio.

* **Problem**:
  This method failed due to **token mismatch** between the encoder output and the LLaMA tokenizer. The token IDs predicted by the audio encoder did not align with the vocabulary space of the LLaMA model, resulting in meaningless or out-of-distribution outputs.

---

### **Method 2: Whisper â†’ Audio Encoder (Emb â†’ LLaMA Emb) â†’ Nearest Token â†’ LLaMA â†’ gTTS**

* **Pipeline**:
  Whisper â†’ Audio Encoder (maps to LLaMA embedding space) â†’ Nearest Token Matching â†’ LLaMA â†’ gTTS

* **Outcome**:
  âœ… This method **worked successfully**. The audio encoder learned to generate embeddings compatible with LLaMA's token space. By mapping them to the nearest valid token before feeding into the fine-tuned LLaMA, we achieved coherent outputs.

* **Diagram**:

![Method 2 Pipeline](Image1.png)



---

### **Method 3: Whisper â†’ Text â†’ LLaMA â†’ Text â†’ gTTS**

* **Pipeline**:
  Whisper (Speech â†’ Text) â†’ LLaMA â†’ Text â†’ gTTS (Text â†’ Speech)

* **Description**:
  This is a more straightforward pipeline using Whisper for transcription and gTTS for synthesis, with the LLaMA model providing intermediate natural language transformation.

* **Diagram**:
![Method 2 Pipeline](Image2.jpg)
## ðŸ“‚ Project Structure

```
â”œâ”€â”€ fine_tune_with_unsloth/          
â”œâ”€â”€ output_audio_files/         
â”œâ”€â”€ sample_input_audio_file/               
â”œâ”€â”€ Image1.png                 
â”œâ”€â”€ Image2.jpg                
â”œâ”€â”€ VB_m2_final.ipynb        
â”œâ”€â”€ VB_m3_final.ipynb        
â””â”€â”€ requirements.txt         
```

---

## ðŸ“Œ Requirements

Install the dependencies using:

```bash
pip install -r requirements.txt
```

---

