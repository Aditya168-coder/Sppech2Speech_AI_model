
# Speech2Speech AI Model

This repository presents multiple methods to train your own **Speech-to-Speech (S2S)** model. 

---

## ðŸ”§ Model Fine-Tuning with Unsloth

We fine-tuned the LLaMA model using [Unsloth](https://github.com/unslothai/unsloth) for efficiency and stability.

### Why we used Unsloth:

* âœ… **Optimized for speed**: It significantly reduces training time.
* âœ… **Memory efficient**: Supports longer sequences and larger batch sizes on limited hardware.
* âœ… **Seamless integration** with Hugging Face and LLaMA models.
* âœ… **Low-Rank Adaptation (LoRA)** support for faster fine-tuning with fewer parameters.

---

## ðŸ§ª Experiments & Hypotheses

We explored three different hypotheses to build the speech-to-speech pipeline:

---

### **Method 1: Whisper â†’ Audio Encoder â†’ LLaMA â†’ gTTS**

* **Pipeline**:
  Whisper â†’ Audio Encoder (Embeddings â†’ Tokens) â†’ Fine-tuned LLaMA â†’ gTTS

* **Goal**: Convert audio to embeddings, translate via LLaMA, and convert back to audio.

* **Problem**:
  This method failed due to **token mismatch** between the encoder output and the LLaMA tokenizer. The token IDs predicted by the audio encoder did not align with the vocabulary space of the LLaMA model, resulting in meaningless or out-of-distribution outputs.

---

### **Method 2: Whisper â†’ Audio Encoder (Emb â†’ LLaMA Emb) â†’ Nearest Token â†’ LLaMA â†’ gTTS**

* **Pipeline**:
  Whisper â†’ Audio Encoder (maps to LLaMA embedding space) â†’ Nearest Token Matching â†’ LLaMA â†’ gTTS

* **Outcome**:
  âœ… This method **worked successfully**. The audio encoder learned to generate embeddings compatible with LLaMA's token space. By mapping them to the nearest valid token before feeding into the fine-tuned LLaMA, we achieved coherent outputs.

* **Diagram**:

```html
<img src="diagrams/method2_pipeline.png" alt="Method 2 Pipeline" width="600"/>
```


---

### **Method 3: Whisper â†’ Text â†’ LLaMA â†’ Text â†’ gTTS**

* **Pipeline**:
  Whisper (Speech â†’ Text) â†’ LLaMA â†’ Text â†’ gTTS (Text â†’ Speech)

* **Description**:
  This is a more straightforward pipeline using Whisper for transcription and gTTS for synthesis, with the LLaMA model providing intermediate natural language transformation.

* **Diagram**:

```html
<img src="diagrams/method3_pipeline.png" alt="Method 3 Pipeline" width="600"/>
```
---

## ðŸ“‚ Project Structure

```
â”œâ”€â”€ audio_encoder/           # Models and scripts for training audio-to-token encoders
â”œâ”€â”€ llama_finetune/          # Scripts for Unsloth-based LLaMA fine-tuning
â”œâ”€â”€ inference/               # Speech-to-speech inference scripts
â”œâ”€â”€ samples/                 # Input/output audio samples
â”œâ”€â”€ diagrams/                # Visual representations of each method
â”œâ”€â”€ VB_m2_final.ipynb        # Experiment notebook for Method 2
â”œâ”€â”€ VB_m3_final.ipynb        # Experiment notebook for Method 3
â””â”€â”€ requirements.txt         # Dependencies
```

---

## ðŸ“Œ Requirements

Install the dependencies using:

```bash
pip install -r requirements.txt
```

---

