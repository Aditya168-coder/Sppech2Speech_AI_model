# -*- coding: utf-8 -*-
"""VB-m2-final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P--dGwqCI4QPY5TwsW6AnMCen5PRj_BE
"""

!pip install datasets torchaudio transformers sentencepiece gTTS huggingface_hub accelerate unsloth git+https://github.com/openai/whisper.git

!pip install huggingface_hub

from huggingface_hub import notebook_login

notebook_login()

from datasets import load_dataset
import torchaudio

dataset = load_dataset("google/fleurs", "en_us", split="train")
mini_fleurs = dataset.select(range(100))

from unsloth import FastLanguageModel
from transformers import AutoTokenizer
import torch

model_name = "aditya168/finetune_GPT"

model1, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model1.to(device)

def generate_text(prompt, max_length=50):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model1.generate(
            **inputs,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            no_repeat_ngram_size=2
        )

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text

def get_embeddings(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        embeddings = model1.get_input_embeddings()(inputs.input_ids).squeeze(0)
    return embeddings

import torch
import whisper
import torchaudio
from whisper.audio import pad_or_trim

whisper_model = whisper.load_model("base")

def get_whisper_embeddings_from_fleurs(audio_sample):

    audio = torch.tensor(audio_sample['array'], dtype=torch.float32)

    if audio_sample['sampling_rate'] != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=audio_sample['sampling_rate'], new_freq=16000)
        audio = resampler(audio)

    if audio.ndim > 1:
        audio = audio.mean(dim=0)

    audio = pad_or_trim(audio)

    mel = whisper.log_mel_spectrogram(audio).unsqueeze(0).to(whisper_model.device)

    with torch.no_grad():
        embeddings = whisper_model.encoder(mel).squeeze(0)

    return embeddings

import torch.nn as nn

class AudioAdapter(nn.Module):
    def __init__(self, input_dim=512, output_dim=3072):
        super().__init__()
        self.adapter = nn.Sequential(
            nn.Linear(input_dim, 1024),
            nn.ReLU(),
            nn.Linear(1024, output_dim)
        )

    def forward(self, x):
        return self.adapter(x)

import torch
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader

adapter = AudioAdapter().to("cuda")
optimizer = torch.optim.AdamW(adapter.parameters(), lr=1e-4)
loss_fn = nn.MSELoss(reduction="none")

def collate(batch):
    audio_embs, text_embs = [], []
    for example in batch:
        try:
            w = get_whisper_embeddings_from_fleurs(example["audio"])
            t = get_embeddings(example["transcription"])
            min_len = min(w.shape[0], t.shape[0])
            audio_embs.append(w[:min_len])
            text_embs.append(t[:min_len])
        except Exception:
            continue

    if not audio_embs:
        return None, None, None

    audio_padded = pad_sequence(audio_embs, batch_first=True)
    text_padded = pad_sequence(text_embs, batch_first=True)

    lengths = [a.shape[0] for a in audio_embs]
    mask = torch.zeros_like(text_padded[..., 0])
    for i, l in enumerate(lengths):
        mask[i, :l] = 1.0

    return audio_padded.to("cuda"), text_padded.to("cuda"), mask.to("cuda")

dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate)

for epoch in range(5):
    for audio_embs, text_embs, mask in dataloader:
        if audio_embs is None:
            continue
        audio_embs = audio_embs.float()
        text_embs = text_embs.float()
        pred = adapter(audio_embs)
        loss_raw = loss_fn(pred, text_embs).mean(dim=-1)
        loss = (loss_raw * mask).sum() / mask.sum()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    print(f"Epoch {epoch + 1} Loss: {loss.item():.4f}")

import torch
import whisper
from whisper.audio import pad_or_trim, log_mel_spectrogram
import torchaudio

def preprocess_audio(audio_path, model):
    audio, sr = torchaudio.load(audio_path)
    if sr != 16000:
        audio = torchaudio.transforms.Resample(sr, 16000)(audio)
    if audio.ndim > 1:
        audio = audio.mean(dim=0)
    audio = pad_or_trim(audio)
    mel = log_mel_spectrogram(audio).unsqueeze(0).to(model.device)
    with torch.no_grad():
        embeddings = model.encoder(mel).squeeze(0)
    return embeddings

@torch.no_grad()
def generate_response_from_audio(audio_path):
    whisper_embeddings = preprocess_audio(audio_path, whisper_model)

    adapted_embeddings = adapter(whisper_embeddings).unsqueeze(0)

    dummy_input_ids = torch.tensor([[tokenizer.bos_token_id]]).to(adapted_embeddings.device)

    outputs = model1(
        input_ids=dummy_input_ids,
        inputs_embeds=adapted_embeddings,
        use_cache=True
    )

    logits = outputs.logits
    next_token = torch.argmax(logits[:, -1, :], dim=-1)
    generated_ids = [next_token.item()]

    for _ in range(100):
        input_ids = torch.tensor([generated_ids[-1:]]).to(adapted_embeddings.device)
        outputs = model1(input_ids=input_ids, use_cache=True)
        next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1)
        generated_ids.append(next_token.item())
        if next_token.item() == tokenizer.eos_token_id:
            break

    response = tokenizer.decode(generated_ids, skip_special_tokens=True)
    return response

audio_path = "/content/gtts_sample_output.wav"
whisper_emb = preprocess_audio(audio_path, whisper_model)

with torch.no_grad():
    adapted_emb = adapter(whisper_emb.to("cuda"))  # [T, 4096]
    adapted_emb = adapted_emb.to(dtype=token_embeds.dtype, device=token_embeds.device)

def embed_to_token_ids(embed_seq, token_embeddings):
    similarities = torch.matmul(embed_seq, token_embeddings.T)
    token_ids = similarities.argmax(dim=-1)
    return token_ids

result = whisper_model.transcribe(audio_path)

token_embeds = model1.model.model.embed_tokens.weight.detach()
recovered_token_ids = embed_to_token_ids(adapted_emb, token_embeds)
recovered_token_ids = recovered_token_ids.unsqueeze(0)
pseudo_inputs = {
    "input_ids": recovered_token_ids.clone(),
    "attention_mask": (recovered_token_ids != tokenizer.pad_token_id).long()
}
messages = [
    {"role": "system", "content": "You are an AI assistant. Your role is to give to the point answers. You need not to answer in more than 100 words"},
    {"role": "user", "content": result}
]

prompt = tokenizer.apply_chat_template(messages, tokenize=False)
true_inputs = tokenizer(prompt, return_tensors="pt").to(device)
if recovered_token_ids.shape[-1] < true_inputs["input_ids"].shape[-1]:
    pad_len = true_inputs["input_ids"].shape[-1] - recovered_token_ids.shape[-1]
    padded_token_ids = torch.nn.functional.pad(recovered_token_ids, (0, pad_len), value=tokenizer.pad_token_id)
else:
    padded_token_ids = recovered_token_ids[:, :true_inputs["input_ids"].shape[-1]]
outputs = model1.generate(
    input_ids=true_inputs["input_ids"],
    attention_mask=true_inputs["attention_mask"],
    max_new_tokens=170,
    do_sample=True,
    temperature=0.3,
    top_k=30,
    top_p=0.85,
    no_repeat_ngram_size=3
)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
response = response.split("assistant")[-1].strip()

from gtts import gTTS
tts = gTTS(text=response, lang='en')

output_path = "test_audio.mp3"
tts.save(output_path)

print(f"Audio saved to: {output_path}")

